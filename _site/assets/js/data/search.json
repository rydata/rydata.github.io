[
  
  {
    "title": "üìä Data Professionals Survey Breakdown - PowerBI Breakdown/Dashboard",
    "url": "/posts/datafield-powerbi/",
    "categories": "PowerBI, Dashboard",
    "tags": "PowerBI, Dashboard, Visualization",
    "date": "2023-12-06 00:00:00 +0800",
    





    
    "snippet": "Image Source: data.org/guides/building-a-data-team/Project Overview  Alex has conducted a survey of over 600 participants to data professionals all over the world. With the raw data coming from the...",
    "content": "Image Source: data.org/guides/building-a-data-team/Project Overview  Alex has conducted a survey of over 600 participants to data professionals all over the world. With the raw data coming from the survey, he tasks a data analyst to create their own visualizations and breakdown of the responses.  Through PowerBI, a breakdown in a form of a dashboard was created to help visualize survey responses.  Initial data was prepared through Power Query before visuals were created into PowerBI Desktop.  Responses such as average salary by job title, preferred programming language, industry of respondents, and ratings on worklife balance are just a few of the responses visualized in the final output.  Huge credits goes to Alex the Analyst for conducting the survey, making it available and providing many resources on data analysis.Here is a preview of the prepared breakdown:You can access the PBIX file created for the project here: LINKYou may also access the dataset used here: LINKObjectives  Create a breakdown and analysis of survey responses taken data professionals.  For data preparation: converted relevant values to numeric values, and apply needed transformations for our visuals.  Formulate and experiment various data visualizations from the available data.  Fix formatting, coloring, labels and visuals for presentation of dashboard analysisImage Source: koenig-solutions.com/blog/beginners-guide-for-learning-microsoft-power-biProject ProcessProject Context  The data set contains survey respondents taken from various Data Professionals (Analysts, Scientists, Engineers etc.) throughout different countries. Data contains information oon salary, satisfaction, preferred programming language, career shifting (and its difficulty) and many more relevant information.  This project focuses more on data visualization and presentation through PowerBI as compared to through data cleaning and preparation. For more data cleaning and preparation related content, kindly check out my other portfolio projects.  Once again, huge credits to Alex the Analyst (YouTube Channel) who created a guide for this project, despite this - personal changes, additional visuals, design changes and measurements were added as I explored PowerBI and its capabilities.Steps Taken1. Imports, Data Transformation and Preparation  Data was imported to PowerBI from an xlsx file and was explored through Power Query.  For roles, ‚Äúother‚Äù jobs listed were removed through the use of split column by delimiter, and replace values.  For salaries, the number ranges were split into two and averaged. Ex. for 66 - 85k, 66 and 85 was split into different columns and averaged. The dash and k were removed using replace values. For max value 225, this was kept. Values were changed from text to numeric.  For countries and industries, ‚Äúother‚Äù options were removed as well.2. Dashboard Analysis Making  Creation of Title and Banner  Cards were created for the amount of respondents and median age to avoid massive massive changes due to outliers.  A stacked bar chart containing the Average Salary by Job Title was made. Proper titling, formatting and labels were added for each visual. Job Title Labels were removed also due to clutter while numbers were added inside bars.  Favorite Programming Langues were visualized along with respondent count and roles through a stacked column chart.  A slider was added to help give a closer look for smaller amounts.  Job title was also repositioned to the right side to allow for more space.  The next visualization initially compared Average Salary by Sex, however, both male and female had very similar salaries with female having slightly higher. This was then replaced to give more room for other visualizations.  Industry of Respondents ideally is shown through a TreeMap, however labels are no longer shown properly. A Pie Chart was shown instead since is a comparison of divisions from the whole set of respondents.  Difficulty to Break into data field was approached similarly.  Pie Charts had to be rotated also to give space for labels.  Originally, this was also a pie chart, however, it took too much space and spaces were visually similar.  A stacked bar chart was then utilized with labels for x and y axis removed. Instead, percentages were included within the bars.  In many cases, a tree map is more effective in visualizing spaces as compared to a pie chart.  Unfortunately Labels do not fit each space, to fix, I transformed data once again to replace United Kingdom to ‚ÄòUK‚Äô and Canada to ‚ÄòCA‚Äô.  Finally, although gauges are rarely utilized, this was a great opportunity to use this visual as it is able to represent a scale well. Initially, a third gauge was added regarding ‚Äúupward mobility‚Äù, this was removed to reduce cluttered. It also only had an average score.3. Assembly of VisualsFinal Visual  Lots of time was put into planning the visuals and colors scheme. Ultimately this scheme was settled on, since, not only was it visually pleasing and worked well with light mode, it was also more accessible for colorblindness (tritanopia).  Overall, visuals were positioned to maximize visibility, uniformity of plot types and visual appeal.Approach 2  Another approach would be to tell a story or  details from left to right, top to bottom like reading a book.  In this Approach: Count of Respondents, Country and Career switch was first introduced to the left, introducing the overall survey.  2nd, Average Salary and industry are positioned in the middle.  Finally, preferred language and salary are put in the right most.  This approach tells more of a story regarding the survey content.  Overall, approaches can be organized in many ways depending on personal preference and flavor.Conclusions and RecommendationsOutput Observations and Insight  To start there are a total of 630 responses, majority from the United States (261), followed by other countries (224). The median age was 28.  Data Scientists are paid the most almost double of Data Analyst. This was followed up by Data Engineer and Data Architect in that order.  Python is massively preferred by most data professionals, followed by R and other. From the dataset, the other options mostly contain SQL.  Surprisingly, 59% shifted into the Data Career field. This may be due to massive factors, including being a relatively new and emerging field.  Majority of survey respondents came from other industries, followed by tech, finance and healthcare.  Almost half found transitioning in the Data field neither difficult nor easy. The rest found it difficult (25%) followed by easy (21%)  Finally, Happiness w/ Salary rating was less than average (4.3/10) and Happiness w/ Work Life Balance was slightly above average (5.7/10).Improving the Project  Since the focus of the project was more on  building the project, more focus can be given into cleaning the datasets. Instead of grouping countries by ‚Äúother‚Äù, more time can be taken to properly transform countries w/ high counts.  This can also be said with Industry and other types of data.  In exchange for clarity, more graphics and visuals can also be added or experimented with.  Although the entire graphic is interactive, filters can also be utilized.  Feature Engineering can also be preformed to come up with new columns and measures to visualize.Thank you for your time reading!"
  },
  
  {
    "title": "üêß Identifying Penguin species through K-means clustering",
    "url": "/posts/penguin-specie-kmeans/",
    "categories": "Machine Learning, Data Science",
    "tags": "Data Science, Unsupervised Learning, K-means, Clustering, Data Preparation",
    "date": "2023-11-30 00:00:00 +0800",
    





    
    "snippet": "Artwork by @allison_horstProject Overview  In most datasets, we do not realistically know how to effectively group or classify our data yet (customer segments, products etc.) or have labeled data a...",
    "content": "Artwork by @allison_horstProject Overview  In most datasets, we do not realistically know how to effectively group or classify our data yet (customer segments, products etc.) or have labeled data already.          When this happens, we can use unsupervised machine learning techniques to classify our data based on commonalities.        Using a K-means model, we were able to automate classifying penguins by its species and genders purely based on their measurements (bill length, bill depth, flipper length body mass and gender).  The K-means model was highly successful in classifying penguins showing us the application of clustering in employee education and projects.You can access the IPYNB file for more details here LINKObjectives  Create a model to find patterns within penguins to allow the zoo‚Äôs care taker team to find patterns to learn more about the penguins specie to give them the proper care and environment needed.  For data preparation: check for missing values, encode data, dropping of columns and scale features to fit our model.  After, create an unsupervised machine learning classification model and identify the most optimal number of clusters or groupings for penguins.  Finally, give recommendations and practical applications for the team based on our findings.Image from British Antarctic Survey: bas.ac.uk/about/antarctica/wildlife/penguins/Project ProcessProject Context  The data set contains a sample size of 345 penguins, classified by species, island, gender, bill and flipper length and depth.  This project was created to focus on the construction of a K-means model rather than in-depth EDA.          For a project with more in-depth EDA process, visit my Human Resource portfolio here: LINK        As a refresher: a K-means model is an unsupervised partitioning algorithm used to organize unlabeled data into groups or clusters. This creates a logical scheme to make sense of data, grouping the data in clusters around central points (or centroids).Steps Taken1. Imports, Data Exploration, Data Encoding and Scaling  Again this will be a short summary of steps taken for the project. You can access the full breakdown and process here: LINKImporting Libraries  Let import the following packages for this project:# Import standard operational packages.import numpy as npimport pandas as pd# Important tools for modeling and evaluation.from sklearn.cluster import KMeansfrom sklearn.metrics import silhouette_scorefrom sklearn.preprocessing import StandardScaler# Import visualization packages.import matplotlib.pyplot as pltimport seaborn as snsFor our dataset, we can import it directly from seaborn using this code block.penguins = sns.load_dataset(\"penguins\")Data Exploration  We use the .head(), .unique(), .value_counts() for our initial exploration.      We learn that we have 152 Adelies, 124 Gentoos and 68 Chinstraps. This gives us additional insight on our desired groupings at the end of the project.    K-means also assumes that there are no missing values. We can check this using penguins.isnull().sum() showing us we have some missing values on some data.  We can drop missing values using the penguins.dropna(axis=0).reset_index(drop=True) function.Data Encoding and Scaling  Within the sex column, some are written as Male/Female instead of MALE/FEMALE.  For consistency, we can transform all to upper case letters using the str.upper() function.  For K-means, we also require the columns to be numeric. We convert sex to numeric using the .get_dummies() function like so:# Convert `sex` column from categorical to numeric.penguins_subset = pd.get_dummies(penguins_subset, drop_first = True, columns = ['sex'])      Finally, since K-means uses distance as a measure of similarity between observations, we have to scale the data using the StandardScaler function. This makes it such that all variables have a mean of 0 and a standard deviation of 1.    We drop the species column as we will not be using it in training the dataset.  Our data is scales using the following code block:#Assign the scaled data to variable `X_scaled`.X_scaled = StandardScaler().fit_transform(X)2. Data ModelingInertia Plot  Going into modeling, we have yet to know how many clusters (k) to use.          To approach this, we may fit the K-means model and evaluating inertia for different values of k.        Let us start by creating a function kmeans_inertia taking in clusters (k) and our scaled values. This returns a list of each k-value‚Äôs interia.  We can then plot this to determine the most optimal clusters.# Fit K-means and evaluate inertia for different values of k.num_clusters = [i for i in range(2,11)]def kmeans_inertia(num_clusters, x_vals):    inertia = []    for num in num_clusters:        kms = KMeans(n_clusters=num, random_state=42)        kms.fit(x_vals)        inertia.append(kms.inertia_)    return inertia# Return a list of inertia for k=2 to 10.inertia = kmeans_inertia(num_clusters, X_scaled)inertiaHere is a plot of the results to visualize the most optimal inertia.In the plot, there is an elbow at point 6, where additional clusters contribute very little changes. We can check silhouette scores to confirm this.Silhouette Score Plot  Start by using the sil_score() function to study the distance between clusters 2-10.  Followed by this, create a function kmeans_sil taking in clusters (k) and our scaled x_vals again to return a list of each k‚Äôs silhouette score.# Evaluate silhouette score.# Write a function to return a list of each k-value's score.def kmeans_sil(num_clusters, x_vals):    sil_score = []        for num in num_clusters:        kms = KMeans(n_clusters=num, random_state=42)        kms.fit(x_vals)        sil_score.append(silhouette_score(x_vals, kms.labels_))            return sil_scoresil_score = kmeans_sil(num_clusters, X_scaled)sil_scoreAgain we can plot these values to show the relationship between number of clusters and silhouette score.To interpreting the plot:  Closet to 1 is the most ideal: telling us that samples are in its own clusters  0 means that samples are near boundaries between clusters  -1 tells us that samples may be in the wrong clustersIn our case, 6 is our ideal value being closest to 1.3. Model Evaluation and VisualizationOptimal k-value  From the previous information, we can fit a six-cluster model to the dataset.  Next, let‚Äôs create a new column adding our cluster labels.  By doing this, our rows are now assigned to each of its own clusters.# adds a new column containing their assigned clusterpenguins_subset['cluster'] = kmeans6.labels_  To verify if they can be differentiated by species, let‚Äôs use a groupby function.# Verify if `cluster` can be differentiated by `species`.penguins_subset.groupby(by=['cluster','species']).size()            Cluster      Species      Count                  0      Adelie      70              1      Gentoo      58              2      Adelie      73              2      Chinstrap      5              3      Gentoo      61              4      Chinstrap      29              5      Adelie      3              5      Chinstrap      34      Likewise, we can utilize a plot to better visualize these.Additionally, we can use another groupby to see if the penguins are sorted by gender to make more sense of our data.# Verify if each `cluster` can be differentiated by `species' and `sex_MALE`.penguins_subset.groupby(by=['cluster','species','sex_MALE']).size().sort_values(ascending=False)            Cluster      Species      sex_MALE      Count                  2      Adelie      0      73              0      Adelie      1      70              3      Gentoo      1      61              1      Gentoo      0      58              5      Chinstrap      1      34              4      Chinstrap      0      29              2      Chinstrap      0      5              5      Adelie      1      3      It can be observed that our model has classified penguins both by species and gender. In addition, it only had 8 wrong classifications (3 adelie and 5 chinstrap).This last visualization shows us how our model classified our data being by species and sex. We can also visualize the small amount of wrongly classified penguins.Conclusions and RecommendationsImage uploaded by Osamu Mustafa: researchgate.net/figure/On-site-images-of-gentoo-P-papua-chinstrap-P-antarctica-and-Adelie-P-adeliae_fig20_318281059Above are on site images of gentoo (P. papua), chinstrap (P. antarctica) and Ad√©lie (P. adeliae) penguins, each adults with chicks.Insight  Adelie had the highest amount of species followed by gentoo then chinstrap.  In the errors, Adelie and Chinstrap females were mixed showing us that they may have more identical traits to each other as compared to gentoo.  Within data science projects, most work are related to preparation (cleaning, encoding, transforming and scaling).  Inertia and silhouette scores are very effective when used together to determine optimal clusters.  Natural groupings was found in the data through our model.  Finally, the groupings into 6 made sense there being 3 species x 2 different genders since different genders and species will have different length, bill depth, flipper length and body mass.Improving the Project  Being an unsupervised model, we do not have access to feature importances. Other types of models can then be explored if you need feature importances. Despite this, it is not as important for our project with the main goal only being to classify penguins.  Although not directly related to this project, aside from classifying by measurements, the company can look into using image classification if it does not have measurements on the penguin.  Despite this suggestion, our model is very practical since penguins may tend to look alike on certain angles.  Additionally we can use further data on their weight and genders in many other applications such as changing the amount of food for each weight class or having a better understanding of the health and changes of the penguins over time.Thank you for your time reading!"
  },
  
  {
    "title": "‚úàÔ∏èüòä Predicting Airline Customer Satisfaction with Machine Learning <br/> (Tree Based Models & XGBoost)",
    "url": "/posts/airline-satisfaction-ml/",
    "categories": "Machine Learning, Data Science",
    "tags": "Decision Tree, Random Forest, XGBoost",
    "date": "2023-11-30 00:00:00 +0800",
    





    
    "snippet": "Predicting Airline Customer Satisfaction with Machine LearningImage Source: https://www.retently.com/blog/airline-satisfaction/Project Overview  3 different supervised machine learning models were ...",
    "content": "Predicting Airline Customer Satisfaction with Machine LearningImage Source: https://www.retently.com/blog/airline-satisfaction/Project Overview  3 different supervised machine learning models were created to predict customer satisfaction  All 3 models built performed greatly in predicting customer satisfaction, these were also able to identify 3 most important predictors.  Decision Tree Model          Scores: F1 95%, Recall 94%, Precision 96%, Accuracy 94%      3 Most important predictors:                  Inflight Entertainment          Seat Comfort          Ease of Online Booking                      Random Forest Model          Scores: F1 95%, Recall 94%, Precision 95%, Accuuracy 94%      The 3 Most important predictors were the same as Decision Tree        XGBoost Model          Scores: F1 94%, Recall 93%, Precision 95%, Accuracy 93%      3 Most important predictors:                  Seat Comfort          Food and Drink / Age          Gate Location                      Recommendations given were focused at improving Inflight Entertainment, Seat Comfort, Ease of Online Booking, as well as improving overall satisfaction through other methods.Access the IPYNB files here:  Decision Tree: LINK  Random Forest: LINK  XGBoost: LINKObjectives  The airline is interested in predicting if customers are satisfied with their services, given customer feedback about their flight experience.  As a data analyst, I am tasked to construct and evaluate a model, and identify which features are most important for satisfaction.  The data contains 129,880 survey responses with data on class, flight distance, and in-flight entertainment, among others.  Basic EDA, data cleaning and preparations are required before building the model.  Recommendations are to be given to improve customer satisfaction.Project ProcessThe overall process for the creation of models are as follows:  Importing packages and loading data  Exploring the data and completing the cleaning process  Building the 3 machine learning models  Tuning hyperparameters using GridSearchCV  Evaluating models using a confusion matrix and feature importances  Providing recommendations for points of focus for the airlineProject Context  The data was used with 3 different machine learning models in separate jupyter notebook files, however, all models will be compiled in this portfolio blog.Decision Tree Steps Taken   Expand for Steps Taken and Model Visualizations   1. Import, Loading and Preparation of the Dataset      Use of functions .head(), .dtypes, .unique(),    There was not much data imbalances with 55% satisfied, and 45% dissatisfied.    There were 393 missing values in Arrival Delay, a separate instance was created and these values were dropped    Satisfaction, Customer Type, Type of Travel and Class were encoded to numerical values for our models using .map() and .get_dummies() functions.    the train_test_split was used to separate 75% of the data for training and 25% for testing.    2. Model Building and Parameter Tuning      As we instatiate the Decision Tree Classifier and fit our data, we get great scores, however this can still be improved through tuning.    Hyperparameter Tuning      After running gridsearch for awhile, we get the best params and scores:    Best Index: 15    Best Estimator: DecisionTreeClassifier(              max_depth=18,        min_samples_leaf=2,        random_state=0)Best Parameters: {‚Äòmax_depth‚Äô: 18, ‚Äòmin_samples_leaf‚Äô: 2}Best Score: 0.9454              F1 Score: 94.54, Recall: 93.59, Precision: 95.52, Accuracy: 94.09  3. Model Interpretation  Confusion Matrix  Let‚Äôs plot a confusion matrix to identify the types of errors made in our model‚Äôs predictions.    Recall that:      Top Left: True Positives - Predicted satisfied and actually satisfied.    Lower Right: True Negatives - Predicted unsatisfied and actually unsatisfied.    Top Right: False Positives - Predicted satisfied but actually unsatisfied    Lower Left: False Negatives - Predicted unsatisfied but actually satisfied.        The model has accurate predictions between the true positives and true negates.    False positives and false negatives also have an equal number, both being generally low compared to the true positives and negatives.    Overall, the model‚Äôs performance is great at identifying a customer‚Äôs satisfaction based on the given predictors.    Decision Tree Visualized  Let‚Äôs plot the decision tree to see how it split the data based on its gini impurity.    To create a more digestible visual, let‚Äôs create a plot for the most important features.        Through this we are able to identify the top factors that lead to customer satisfaction being              ‚ÄòInflight entertainment‚Äô,        ‚ÄòSeat comfort‚Äô,        ‚ÄòEase of online booking‚Äô.              The model has a good performance and accurately predicted satisfaction 94% of the time.    The confusion matrix also shows a good performance and an equally low amount of false positives and negatives in relation to the true positives and true negatives.    Recommendations will be given at the end  Random Forest Steps Taken   Expand for Steps Taken and Model Visualizations   1. Import, Loading and Preparation of the dataset      Same as before:    Use of functions .head(), .dtypes, .unique(),    There were 393 missing values in Arrival Delay, a separate instance was created and these values were dropped    Satisfaction, Customer Type, Type of Travel and Class were encoded to numerical values for our models .get_dummies() functions.    the train_test_split was used to separate 75% of the data for training and 25% for testing.    2. Model Building and Parameter Tuning      For the best model performance, we set some hyperparameters for tuning the model using GridSearchCV    Create a list of split indices using PredefinedSplit() to identify points from the train set will be treated as validation data during GridSearch.    After fitting the model, we obtain our most optimal parameters:              {‚Äòmax_depth‚Äô: 50        ‚Äòmax_features‚Äô: ‚Äòsqrt‚Äô,        ‚Äòmax_samples‚Äô: 0.9,        ‚Äòmin_samples_leaf‚Äô: 1,        ‚Äòmin_samples_split‚Äô: 0.001,        ‚Äòn_estimators‚Äô: 50}              3. Model Interpretation      Using our best parameters, we fit the optimal model and predict on test set.    Here is our tuned model‚Äôs scores on the test set compared to the decision tree                      Model        F1        Recall        Precision        Accuracy                            Tuned Decision Tree        0.95        0.94        0.96        0.94                    Tuned Random Forest        0.95        0.94        0.95        0.94              We can also plot the feature importances to have an idea of the most important predictors as identified by our Random Forest.        Like our decision tree, the top 3 most important features are Inflight Entertainment, seat comfort and ease of online booking.    Compared to our decision tree, the random forest is marginally better at 3 scores, only by 0.01-0.002. Precision is better with our decision tree but only by 0.005.  XGBoost Steps Taken   Expand for Steps Taken and Model Visualizations   1. Import, Loading and Preparation of the dataset      Same as before:    Use of functions .head(), .dtypes    Satisfaction, Customer Type, Type of Travel and Class were encoded to numerical values for our models .get_dummies() functions.    One of the advantages of Gradient boosting machines is that it works well with missing data. Missing data is information. We will no longer eliminate null values for this.          the train_test_split was used to separate 75% of the data for training and 25% for testing.            2. Model Building      We start by instantiating our XGBClassifier, setting parameters for our GridSearch and defining our scoring metrics.    We then fit the grid search to our training data.    We obtain our best parameters:              Best Parameters:        ‚Äòcolsample_bytree‚Äô: 0.7        ‚Äòlearning_rate‚Äô: 0.3        ‚Äòmax_depth‚Äô: 6        ‚Äòmin_child_weight‚Äô: 3        ‚Äòn_estimators‚Äô: 15        ‚Äòsubsample‚Äô: 0.7              Best Accuracy:  93.94%  3. Model Evaluation  Here are the compiled scores for all 3 models.                    Model        F1        Recall        Precision        Accuracy                            Tuned Decision Tree        0.95        0.94        0.96        0.94                    Tuned Random Forest        0.95        0.94        0.95        0.94                    Tuned XGBoost        0.94        0.93        0.95        0.93                  Precision of 94.72% tells us that the model is good at predictive true positives or satisfied customers.    Recall of 93.31% lets us know that the model does a good job of correctly identifying dissatisfied passengers within the dataset.    F1 score balances both precision and recall. With 94.01% f1, this is a strong predictive power in the model.    Similar to our Random Forest, we can utilize a Confusion Matrix to visualize the predictions of our model.        The True Positives and True Negatives in the top left and bottom right are very high with 13000 and 16000 respectively, in relation with the False Positives and False Negatives, this helps us visualize why the is accuracy and other metric scores is high.          Unlike our previous models, this one has different features identified as the most importance:              Seat and comfort, food and drink, and age was the most important here, followed by gate location and departure time.            Conclusions and RecommendationsAgain, here are the compiled scores for all 3 models:            Model      F1      Recall      Precision      Accuracy                  Tuned Decision Tree      0.95      0.94      0.96      0.94              Tuned Random Forest      0.95      0.94      0.95      0.94              Tuned XGBoost      0.94      0.93      0.95      0.93      Insight  In terms of model scores the Random Forest performed the best, followed by Tuned Decision Tree then the Tuned XGBoost. Despite this, differences are very marginal only by 0.01.      All 3 models were highly effective and predictive in passenger satisfaction.    For both the Decision Tree and Random Forest the 3 most important predictors are:          Inflight Entertainment      Seat Comfort      Ease of Online Booking        For the XGBoost Model the 3 most important predictors are:          Seat Comfort      Food and Drink / Age      Gate Location        As such, recommendations will revolve around improving these.Recommendations      From these insights, further research may be done to identify which has the best cost / improvement ratio when it comes to improving seat comfort or food and drink and so on.        Focusing on creating good inflight entertainment through improved media on airport seats (if available), or interviewing customers on what they liked the most In terms of inflight entertainment may be rewarding for the company when it comes to increasing satisfaction.        Seat comfort can be improved through investing in back rests or small pillows, improving scent or cleanliness of seats and investing in better seats for future plane selection.        In terms of Ease of online booking, A/B and statistical tests can be done to determine whether or not certain versions of the websites had faster checkouts or better conversion. Investing into UI and web development may be beneficial for the airline.        For Food and Drink, investing more in identifying customers preferences (which may differ for each location) may prove beneficial to overall satisfaction. Investing in quality ingredients, food R&amp;D, and conducting surveys may play a big role in improving overall satisfaction.        Finally, the models are highly predictive in all metrics. Further investing into building and understanding the models will be a huge asset to the company by enabling them to do interventions/additional programs for unsatisfied clients and utilizing the model for future testing if client satisfaction improved with the recommended changes.  Thank you for your time reading!"
  },
  
  {
    "title": "üÜéüß™ Significance Testing and Statistical Analysis on Taxi Revenue",
    "url": "/posts/stat-analysis-ab-testing/",
    "categories": "Statistics, ABTesting, NYCTLC-Series",
    "tags": "Statistical-Analysis, AB-Testing, Descriptive-Analysis, Python",
    "date": "2023-11-30 00:00:00 +0800",
    





    
    "snippet": "A/B Testing and Statistical Analysis for NYC TLC Fare PaymentEmpire Hotel in NYC Taken from an episode of Gossip Girl. Image Source: thenotsoinnocentsabroad.com/blogProject Overview  The A/B Testin...",
    "content": "A/B Testing and Statistical Analysis for NYC TLC Fare PaymentEmpire Hotel in NYC Taken from an episode of Gossip Girl. Image Source: thenotsoinnocentsabroad.com/blogProject Overview  The A/B Testing conducted showed a statistically significant difference in payment amount between cash and credit card payments.  Credit Card total payments was significantly higher than cash payments.  Further analysis shows that the difference might be due to tips.  A second  A/B test was conducted without the tips, showing that there is no statistical difference in fare amount, when tips were not accounted.  Using the groupby function, we see that tips are only recorded for credit card payments.  Recommendations were given to incentivize logging of tips.You can access the JUPYTER Notebook IPYNB file here LINKObjectives  NYC TLC requests an analysis of the relationship between fare amount and payment type (cash vs credit card payments) via A/B testing.  Answer: Does different types of payment methods (cash vs credit card) amount to a statistically significant difference in total amount paid?  Provide recommendations to increase revenue for taxis.Project ProcessProject Context  This output is part of a multiple series of outputs for New York City Taxi &amp; Limousine Commission (NYC TLC)  This is a simpler project more focused on gathering insight and analysis than technical process (as compared to our HR Analysis Output).Steps Taken1. Data Exploration  use of .head, .describe, .shape and .groupby for initial analysis on our data.# grouping payment type and total amounttaxi_data.groupby('payment_type')['total_amount'].mean()            payment_type      mean_total_amount                  1 (Credit Card)      17.66              2 (Cash)      13.55              3 (No charge)      13.58              4 (Dispute)      11.24              5 (Unknown)      None        From here we can see that credit card payments are much higher than other types of payments.  This difference might be from random sampling rather than true difference. To investigate this, we can conduct a hypothesis test.2. Hypothesis testing (Round 1)  Our goal is to conduct a two-sample t-test.Identify our null and alternative hypothesis:      H0: There is no difference in the average total fare amount between customers who use credit cards and customers who use cash.        HA: There is a difference in the average total fare amount between customers who use credit cards and customers who use cash.  Choose a significance level:      We choose a 5% significance level for our two-sample t-test. 5% is usually the industry standard and is requested for this project.        We use the ff code block to conduct our test:  Finding the p-value:We use this code block to use scipy‚Äôs stats hypothesis testing library with the stats.ttest_ind() function.# conducting our hypothesis testcash = taxi_data[taxi_data['payment_type']==2]['total_amount']credit_card = taxi_data[taxi_data['payment_type']==1]['total_amount']stats.ttest_ind(a=credit_card, b=cash, equal_var=False)Ttest_indResult(statistic=20.34644022783838, pvalue=4.5301445359736376e-91)  Assuming a significance value of 5%, we can reject the null hypothesis, we see that there is a statically significant difference from cash and payment method.3. Hypothesis Testing Interpretation (Round 1)We can conclude that there is a statistically significant difference in the average total fare amount credit card vs cash users.Insight!Clarifications has to be made regarding the larger payment for credit cards. It is recommended to look into the following:  First, is there a system bug that causes credit card users to pay more? What may have caused these?  Second, it may be insightful to also look at the average amount of monthly rides for credit card users.          Do credit card users tend to be more ‚Äúpower users‚Äù?      Meaning are they the type of people that spend more on rides or go in longer amount of rides?        Third, is looking at the differences that credit card offers.          Are there any ongoing promotions for credit card users?      Does the hassle free and cashless nature of credit card important to the riders?      Img Source:www.moneymax.phHypothesis Testing Part 2Another comparison is the comparison of the fare amount, which does not include tips, only the base payment.A second round of a/b testing can be looked into to answer this.1. Data Exploration (Round 2)Instead of total amount paid, we use the fare amount to exclude tips.# Grouping payment type and fare amount meantaxi_data.groupby('payment_type')['fare_amount'].mean()            payment_type      mean_total_amount                  1 (Credit Card)      13.43              2 (Cash)      12.21              3 (No charge)      12.19              4 (Dispute)      9.91              5 (Unknown)      None      Removing the tips, we see that there is not as much difference anymore between cash and credit card payments.2. Hypothesis Testing (Round 2)Identify our null and alternative hypothesis:      H0: There is NO difference in the average fare amount (excluding tips) between customers who use credit cards and customers who use cash.        HA: There is A difference in the average fare amount (excluding tips) between customers who use credit cards and customers who use cash.  Choose a significance level:  In this case we retain 5% as our significance levelFinding the p-value:We apply A/B testing with the same code block:cash_fare= taxi_data[taxi_data['payment_type']==2]['fare_amount']credit_card_fare = taxi_data[taxi_data['payment_type']==1]['fare_amount']stats.ttest_ind(a=credit_card_fare, b=cash_fare, equal_var=False)Ttest_indResult(statistic=6.866800855655372, pvalue=6.797387473030518e-12)  Assuming a significance value of 5%, we can see that there is no statically significant difference from cash and payment method.3. Hypothesis Testing Interpretation (Round 2)  Although further analysis on the possible ‚Äòbug‚Äô on higher amount for credit card users, this can be ruled out as there is less difference comparing the mean fare amount for cash and card.We can do another groupby on payment type and tip_amount by its mean.            payment_type      tip_amount                  1 (Credit Card)      2.73              2 (Cash)      0              3 (No charge)      0              4 (Dispute)      0              5 (Unknown)      None        We can observe that no tips are reported during cash payments, only at credit card.Conclusions and RecommendationsImage Source: https://www.blackandwhitecabs.com.auInsight  It can be said that the higher amount for credit cards is due to the fact that tips are not recorded anymore by drivers for cash payments.          It is worth investigating if the company takes a percentage of tips from the riders, disincentiving them from recording tips.        Additionally, it is possible that when paying with credit card, there is an option or requirement within the application to tip, leading to more pay.  Another explanation is that riders do not carry as much cash, so it is easier to pay with credit card for long trips.          In this case, fare amount determines payment type rather than vice versa.      Recommendations  Since it can be seen that tips are no longer reported on the system on average compared to credit card payment methods. It is possible for the team to look into methods to further incentivize recording of tips given, through the ff:          Performance Bonuses      Recognition and Rewards      Education and Training      Transparent Communications and Policies      Technological Systems Feedback      Further discussion with drivers        Finally, conducting additional analysis on what causes tipping through interviews, surveys, additional data or FGD can be a viable step for the company.Thank you for your time reading!"
  },
  
  {
    "title": "üè† Real Estate House Sales - Tableau Dashboard",
    "url": "/posts/tableau-project/",
    "categories": "Tableau, Dashboard",
    "tags": "Tableau, Dashboard, Visualization",
    "date": "2023-11-23 00:00:00 +0800",
    





    
    "snippet": "Real Estate House Sales Dashboard (Tableau)Project Overview  The dashboard represents House Sales for King County, Washington and contains data on house details (sqft, floors, rooms, yrs), prices c...",
    "content": "Real Estate House Sales Dashboard (Tableau)Project Overview  The dashboard represents House Sales for King County, Washington and contains data on house details (sqft, floors, rooms, yrs), prices conditions and geodata (latitude/longitude, zipcode).  6 visualizations were created,          Daily House Sales Price (Line Chart)      House Price Distribution (Bar Plot)      Bedroom Count Distribution (Bar Plot)      Bathroom Count Distribution (Bar Plot)      View vs Condition (Heatmap)      Geodata Map        This is paired with interactive filters on:          Date      Year Built      Square foot Lot and Living      House Grade        This project is mostly based on familiarizing myself with the functions of tableau, however, it can be very helpful for buyers, and real estate sellers to have an overview of prices to expect depending on the property.  Viewers are able to see the prices of houses based on the year, size, rooms and location to get an idea of how to price their lots, or what prices to expect when purchasing properties.  Finally huge credits goes to ‚ÄúData With Mo‚Äù youtube channel for providing these and additional resources teaching Data Analytics. Despite the hands-on guide, I added additional functionality and edits (such as the map filter).Please access the full Tableau Dashboard here LINK.You can access the data used in XLSX format here LINK.Objectives  Create a dashboard that visualize changes in Home Sales Price by time, in relation to its traits (bedroom/bathroom count, year built, size) and locations.  Provide an overview of distribution of prices and rooms.  Visualize the condition and view of houses with its corresponding prices.  Create interactive filters for house buyers and sellers to know more about average house prices in accordance to their needs.Project ProcessProject Context  This project was specifically undertaken to fill in gaps in my knowledge of creating visuals after completing the Google Advanced Data Analytics Certificate.  Although the mentioned program went in depth in creating visualizations, I still felt the need to take more projects to familiarize myself further with tableau, as I have already done a dashboard in excel (see my excel dashboard here).  Although made in Tableau, I am also currently taking a PowerBi course (Story Telling with PowerBi - Coursera) and will update this portfolio website when more time is available.Steps Taken   Expand for full tableau creation process:  1. Loading of Data Set, Validating Data, Formatting      Data set was loaded in tableau. Fortunately, this dataset has already been cleaned and validated. This is a rarely the case in actual scenarios, but this is okay as the goal of this project is to learn tableau    Data types had to be properly validated/set each time they were dragged to columns and rows for the visuals.    Proper formatting had to be applied for each visual: font, colors, titles, lines and visualizations had to be uniformed.    2. Creation of Visuals      Visual 1 - Avg House Sales Price Line Chart: data types had to be validated and formatted.    Visual 2 - GeoData Map: proper country and zipcode had to be identified. Map was formatted for visibility.    Visual 3 - Distribution of House Prices: Proper identification of data types and formatting was applied. Tooltip fixed.    Visual 4 - Distribution of Bedrooms: formatting pasted from previous visuals. Renaming title and sheet.    Visual 5 - Distribution of Bathrooms: formatting was pasted and adjusting of content. Tooltip corrected.    Visual 6 - View vs Condition Heatmap: Dragging and reorganizing of variables, sorting fixed manually. Dragged price to color and labels. Formatted titles and visuals again.    3. Creation of Filters      Create calendar filter: drag calendar to col (weekday), rows (week) and filters (month/year). Drag price to color (avg), date to label (day) and format colors and columns.    Year filter: drag year to filters, show filter.    Sqft living/lot filter: drag sqft living/lot to filters, show filter.    4. Build the dashboard      Fixing sizing, set minimum to generic desktop size, uncheck maximum.    Drag necessary items: containers, text, object, filters, and rename titles.    Drag all visualizations and finalize formatting.    5. Filter functionality      The dashboard can be seen in the first sheet, while the second sheet contains instructions, input cells and calculations for the dashboard.    For future references and projects, this calculation sheet design can and should be improved. However, only the main visual was the focus during the task.    Applied filter by clicking:              More options &gt; apply to worksheets &gt; selected worksheets (select all).        Dashboard &gt; actions &gt; Filter &gt; edit &gt; untick map and line chart.        Do the same for the map to make it a filter.        Do the same for Yr, Sqft Loft and Sqft Living filters.              6. Final Check and Publishing      Final check up and formatting of text, numbers and relevant data.    Test sliders and filters.    The dashboard are saved and published via Tableau Public.  Conclusions and RecommendationsInsight  From May 2014 - 2015 (entire dataset), the highest distribution of houses had the following attributes:          Average Price of USD 500K      Average to Good Condition paired with Good to Excellent View      3-4 Bedrooms and 1-2 Bathrooms      Prices indicators:  Sqft living is the one of the highest indicators going beyond USD 1,000K when looking at the top half of largest sqft living.  ‚ÄúGrade‚Äù is also one of the highest indicators.          Grades 1-7 costs sits mostly USD 250K      At Grades 8 - 12, prices drastically jump to at average of USD 500-600K      Recommendations for House Shopping  For price sensitive buyers with budget around USD 250K, look at houses with grades from 1-7.  For those that want improved houses, it may be worth looking at lower grade houses and investing in renovation instead.  To make the most out of 300K budget, one can look  at houses built recently at 2000, and even look at grades 7-12. This seems to be the sweet spot for maximizing budget to house condition.  Despite the recommendations, shopping or selling for a house has lots of factors to account for, however, the dashboard is still able to give us an idea of appropriate pricings based on location.Improving the Project  Although the main purpose of this project to practice tableau was met, the following can be done to better the project:          Use more recent dates, or an API to gather up-to-date prices of homes.      For buyers and sellers, having a price filter will also be useful.      If data is available, additional attributes about the house can also be visualized.      With feature engineering, one can make a score rating of the amount of nearby establishments (schools, stores, offices) in relation to distance. This gives us a score for the locations which is especially beneficial for buyers and developers.            More thought on improving the visual can be put, however, this one retains good visibility and clarity.    Again you may access the full Tableau Dashboard here LINK.Thank you for your time viewing!"
  },
  
  {
    "title": "üìà Banking App Enrollment Campaign - Excel Dashboard",
    "url": "/posts/excel-project/",
    "categories": "Excel, Dashboard",
    "tags": "Excel, Dashboard, Visualization",
    "date": "2023-11-23 00:00:00 +0800",
    





    
    "snippet": "Enrollment Campaign for Banking App RegistrationProject Overview  A bank is launching a new campaign aimed to encourage current clients to enroll in their new mobile application.  The country is di...",
    "content": "Enrollment Campaign for Banking App RegistrationProject Overview  A bank is launching a new campaign aimed to encourage current clients to enroll in their new mobile application.  The country is divided into 5 regions. Performance for each region can be filtered using slicer function in excel.  A line graph, stacked bar chart and highlights of key values were used to represent key factors.  Data are inputted to a different sheet and values automatically update in the dashboard.You can access the EXCEL file here LINKObjectives  Visualize overall enrollment and performance for each region.  Compare registered and unregistered clients for each region.  Give information on campaign health via:          Current Enrollments vs Target      Total this Week and Remaining Clients      Days Left      Registered Clients (at 2.5M, at 2.5M and vs last week)      Project ProcessProject Context  This dashboard was given to me as a task for one of my internships, however, at that time I was not knowledgeable in the data field yet.  The task was to create a dashboard to track performance for the new mobile app enrollment campaign.  Due to data privacy, I was not provided with data, only examples from previous dashboards.Steps Taken1. Gathering of Relevant information  Available resources, tasks, references and relevant data was gathered in this step.  One of the most important resources I had was the reference to old dashboards due to the lack of data access as interns.2. Communication with relevant stakeholders  Despite the limited data access, my mentor at that time tasked me to create a format for the dashboard. I volunteered creating it via excel, including a backend sheet that automatically updates the dashboard based on values inputted.3. Creation of Data and Dashboard  Creating dummy data while considering if it can represent actual values was an important skill set in this case, as I had to understand how each value affects the overall data and visualization.  The dashboard can be seen in the first sheet, while the second sheet contains instructions, input cells and calculations for the dashboard.  For future references and projects, this calculation sheet design can and should be improved. However, only the main visual was the focus during the task.4. Dashboard and Visual DesignsVisual 1  The design and layout of the dashboard was created using canva. There were several drafts and iterations throughout designing but it was important to stick to the branding colors and visuals.  This was the visual used for the output with the logo removed.Visual 2  For the sake of the portfolio, I did slight edits of how I would approach the design. Despite this, I have kept the same functionality and visualizations.  The uploaded excel file will be using this visual.Conclusions and RecommendationsInsight  Although this visual used synthetic data, here are insights that can be gathered from the campaign:  The current registrations has kept up with the target registrations at the start but slowed down overtime.  South Metro Manila (SMM), North Metro Manila (NMM) and Visayas Mindanao (VISMIN) have the highest amount of registrations respectively whereas South Luzon (SL) has the highest growth versus last week.  All 5 divisions are growing at a steady rate. Performance dipped on week 5 for all divisions - in contrast to SL with a performance increase.          It is worth investigating what SL did during week 5 to increase registrations.      At 1.5Mn registrations, the number of clients registered was only 23% of the target set.      Performance improved and at 2.5Mn registrations, registered clients reached 76% of the target.      Although interpretation on this has to be clarified, compared to last week, current registration are still low only hitting 37% of the target registration.      Improving the Project  For future purposes, it will be ideal to link values from an SQL database to a visualization application such as Tableau or PowerBI.  However, if stakeholders or future users are more familiar with excel, excel is still very much a good option.  The visual was inspired by ‚Äòdark mode‚Äô, but in terms of accessibility, brighter takes will be easier to see.  If data was present, more filters/slicers to visualize results by date will be a great addition.  Feature engineering may also be also be applied to come up with new columns and further visualizations.Thank you for your time reading!"
  },
  
  {
    "title": "üßëüèª‚Äçü§ù‚ÄçüßëüèΩ HR Analysis: Predicting Employee Turnover with Machine Learning <br/> (EDA, Regression, Tree Based Models & XGBoost) - Full Process",
    "url": "/posts/hr-analysis/",
    "categories": "Full Process, Data Science",
    "tags": "Data Science, Cleaning, Eda, Visualization, Logistic Regression, Decision Tree, Random Forest, Feature Engineering, Recommendations",
    "date": "2023-11-23 00:00:00 +0800",
    





    
    "snippet": "Image Source: peoplemanagingpeople.comData Science ProcessIn this project, the following skills were applied: Cleaning, EDA, Machine Learning Models (Logistic Regression, Decision Tree, Random Fore...",
    "content": "Image Source: peoplemanagingpeople.comData Science ProcessIn this project, the following skills were applied: Cleaning, EDA, Machine Learning Models (Logistic Regression, Decision Tree, Random Forest) and Recommendations  Please use the outline on the right side of the webpage for easier navigation!Project Objective Overview  Central Question: Why causes employees to leave the company?  We are tasked to understand the employee turn over in the company.  Based on this, we pull insights and strategical recommendations.  Libraries used include: numpy, pandas, matplotlib, seaborn, sklearn (logistic regression, decision tree, random forest, gridsearch, metrics) and pickle to save the model.  You can access the Jupyter Notebook IPYNB file here LINKProject Findings OverviewOn Employees who stayed:  Staying employees had an average of 3-4 projects assigned, rendering monthly hours of 150-255 (still high compared to average of 167hrs)  Satisfaction levels were generally high but lowered for tenure/years 5-6.On Employees who left:  Employees are generally overworked with almost double the regular monthly hours.  Those who left either worked less hours and had only 2 projects assigned OR  Are overworked with 255 - 300+ hours and had had more projects (4-7 projs assigned).  Everyone with 7 projects assigned left.  Highly satisfied employees, with more than 5 years are leaving due to lack of promotion and salary increase.  Those with 4 years in the company, may have left due to drastic policy change.On Regression and Machine Learning Models:  The logistic regression model approach achieved high scores (79% precision, 82% recall, and f1 of 80%).  The decision tree model and random forest had even better scores and performed similarly (94% precision, 91% recall, f1 of 93%, accuracy and auc 98%).  Feature Engineering was used to drop data which will not be realistically available.  Second round of tree models were created. The random forest is the champion model and scored very well on the test set: (87% precision, 91% recall, 89% f1, accuracy 96% and auc 94%)Recommendations at the end!Data Cleaning and PreparationAfter importing the packages we can do our initial data cleaning  head(), .info, .describe and .columns to better understand our data  rename the columns to snake_case (lowercase and underscores) and correct mispellings  .isnull(), .duplicated(), .drop_duplicates() to check for missing values and duplicates  boxplot to check for outliersWe use Interquantile Range to identify the outliers. Fortunately there were no null values, however, there were about 3000 duplicates and 824 outliers in the tenure section. There are no major ethical considerations at this stage however we need to consider whether or not to remove outliers.EDA and VisualizationUsing the .value_counts(normalize=True) function, we can see the ratio of leavers. 1991(17%) left and 10000(83%) stayed.Data VisualizationsFirst, we use a stacked boxplot from the seaborn library to show the average monthly hours, compared to number of projects. We used stacked to visualize those who left (marked as 1) and those stayed (marked 2).1. Monthly Hours by Number of Projects (Stacked Box plot)From here we can observe that leavers fall into certain groups ff:Group A:  Assigned only 1-2 projects, this has the highest amount of leaves. These group worked significantly less hours than peers.  It is possible these were fired due to lack of hours, or were assigned less since they have filed LOA, vacation or leave notices.Group B:  Assigned 4-7 projects, as projects increase, so does monthly hours, and leaves.  Everyone assigned 7 projects left.  Worked significantly more hours and are overworked.  Worked 255 - 295 hours for a month. Double the usual amount of 166 hours (8 hrs, 5x a week, 4x a month).Stayed Group:  Assigned 3-4 projects, working 150 - 255 hours, had the highest amount of stays.  Compared to the average 166 hours, this group still works a significant amount of hours.2. Monthly Hours and Satisfaction level (Scatter Plot)  This scatter plot examines average monthly hours and satisfaction levels.  The dotted red line shows us a reference point of an average 166 hours for the average employee hours.  Notice how leavers (in orange) are clumped in certain areas of the scatter plot. This backs up our observation on Group A and Group B as mentioned earlier.  Group C: Looking at the top right of the plot, we notice a third group not evident in the first box plot.          This group has high satisfaction (0.8) but paired with high work hours (225-275), ended up leaving.        Note that we also see a strange shape of distribution, showing signs of synthetic data as distributions are not evenly distributed but instead are clumped up. They are also skewed to the right.3. Satisfaction by Tenure (Stacked Box plot)Let‚Äôs continue using our grouping system to make general observations. We can compile them later. From this visual we can observe the ff:  2 years had generally low satisfaction (Group D)  3 years had low satisfaction and highest leaves (Group D)  4 years has set of dissatisfied employees who left (Group F)  5-6 years had high satisfaction, but left (Group E)Employees who left fell into two categories:  Group D. Dissatisfied with lower tenures and  Group E. Very satisfied with medium to higher tenures.          Further investigation should be done. Why did the employees leave despite a high amount of satisfaction?      It is possible that although they enjoyed their work, lack of promotions or salary increase caused them to leave.        Group F. 4 Year tenure, with very low satisfaction.          It is worth investigating further that led to very low dissatisfaction levels. Did this group face major changes in policies or salaries?        Satisfaction levels of year 7-10 and new employees are similar. Looking into which reasons led them to stay will be valuable.A closer look will be applied for Group E as it is questionable why they left despite high satisfaction.4. Group E: Employee and Salary Distribution (Pie Chart)  To analyze this we filtered the group by satisfaction level of 7 or more, tenure of more than 5 and left the company.  Despite being in the company for 7+ years, only 2% had high salaries. 58% have low salaries, and the rest are in medium.      28% are in sales, 19% in IT and 17% in support. Only 1 person was promoted.    Despite the high satisfaction, leaves may be caused by lack of promotion and increase in salary despite the amount of years in the company.5. Mean and Median Analysis on Leaves and Stays (Table)  We use this code block to show a table depicting the mean and median satisfaction of employees who left and stayed:# Calculate mean and median satisfaction scores of left and stayeddf1.groupby(['left'])['satisfaction_level'].agg([np.mean,np.median])            ¬†      Mean      Median                  Left      0.677      0.69              Stayed      0.440      0.41      For those who stayed (Mean &lt; Median):  Distribution is negatively skewed  There are more extreme scores (dissatisfied) in the bottom than at the top (satisfied)  The majority are only moderately satisfied, but there is a small number very dissatisfied pulling down the mean.  The company may have a polarized environment - meaning there are very satisfied and very dissatisfied workforceFor those who left (Mean &gt; Median):  The most dissatisfied employees are more likely to leave, the company looses its unhappiest employees, improving overall satisfaction in remaining, this may be a flag of high turn over rates.  There are root issues not addressed leaving to more leaves.6. Salary Histogram by Tenure (Short and Long, Histogram)  As years increase, the salary does not increase proportionally.  Longer years did not necessarily convert to higher salary.  For those that are paid high salaries, they have stayed in the company for more than 10 years. This may refer to C-level managers.7. Monthly hours by evaluation score (Scatter Plot)  Leavers (orange) are observed in the lower left and upper right of the plot.  Two leave groups can be found again:          High monthly hours and received high evaluation      Less time below the average monthly hours and received low evaluation        Working long hours does not guarantee high evaluation score since evaluation is still distributed randomly across increasing hours  Again, large majority of employees work more than 167 hours a month.8. Monthly hours by promotion (Past 5 years, Scatter Plot)The plot informs us the following:  The top part shows those who received promotions. Those below did not.  The proportion of promoted in the last 5 years were small in relation to the amount of employees.  Most of those who left were from those not promoted, despite working the longest hours.9. Employee Leaves by Department (Histogram)  No department in particular has a proportionally amount who stayed or left.  Although, sales, technical and support has the highest amount who left - this is balanced by the fact that there are more employees as well.10. Variable Correlation (Heatmap)The highest positive correlation includes:  Number of projects and average monthly hours  Last evaluation and number of projects  Last evaluation and average monthly hoursThe highest negative correlation includes:  satisfaction level and employees leftEDA Insight Summary!Various observations can be said through our eda, we found that leaving is related with:  longer working hours  increasing amount of projects  lower satisfaction levels  lack of promotion  lack of increase in salary throughout years.Working long hours, but not receiving promotions, higher salary, or better evaluation scores may have caused dissatisfaction and burn out. Despite this, people who have stayed for more than 6 years tend to stay.   Expand for additional insights:  To decrease confusion of our alphabetical groupings, we will compile them into 4 main categories of ‚ÄúChurned‚Äù.  Churned 1 includes:      Group A: Significantly worked less hours with only 2 projects assigned    Group D: dissatisfied and with lower tenure, 3 years and below    This subgroup have lower tenures, worked less hours and had less projects assigned.    Churned 2 includes:      Group B: Significantly more hours worked and more projects assigned    Everyone with 7 projects assigned left    This subgroup may have been exposed to overwork or experienced burnout.    Churned 3 includes:      Group C: High satisfaction, but very significant amount of work hours 225 - 275.    Group E: High satisfaction, with higher tenure (more than 5 years) but still left    This subgroup may have left due to lack in promotion and salary increase.    Churned 4 includes:      Group F: 4 Year tenure with very low satisfaction    This subgroup may have experienced impactful changes.    Further investigation of changes in policies required.    Stayed:      For the groups who stayed, there was an average of 3-4 projects assigned.    High monthly hours of 150-255 hours, from the average 166.7    Satisfaction level ranges were relatively high for tenures 2-4, lowered for tenures 5-6, then returned to the initial ranges for 7 and above years.  Model Building A - Logistic Regression  üß™ üìàThe following is a concise outline of steps taken, however, we will not be going into detail.Again, feel free to visit the jupyter notebook for step by step information here LINK   Expand for Steps Taken:      Encode non-numeric variables (via pd.get_dummies function)    Correlation Heatmap    Employees Left / Stayed across Depts.    Remove outliers    Feature Selection    Test, train split    Fitting the model    Confusion Matrix    Checking class balance    Evaluation of Metrics  Logistic Regression Model EvaluationTo evaluate a model we can use a Confusion Matrix to show us the amount of True Positives/Negatives and False Positives/NegativesCorrect Predictions:  Top Left: True Positives - Predicted leaving and actually left.  Lower Right: True Negatives - Predicted staying and actually stayed.False Predictions:  Top Right: False Positives - Predicted leaving but stayed  Lower Left: False Negatives - Predicted staying but left.What this means  The model was able to correctly predict the majority of employees leaving (2165).  The next largest quadrant is False Negatives, meaning the model predicted leavers, who actually stayed.  This means that our model has a little bit of overfitting, as it is sensitive to factors that would make employees leave, this is good since the model is  sensitive to employee that has potential to leave.Scoring  With 83% and 17% split, the balance is not perfect, but not too imbalanced also, this may have cause the slight overfit.      Due to this accuracy is not a good metric, but we can use precision, recall, f1 and support.    The logistic regression achieved a precision of 79%, recall of 82%, and f1 of 80%.Model Building B - Decision Tree and Random Forest ‚öóÔ∏èüèïÔ∏èThe following is a outline of steps taken throughout the Decision Tree modeling. For full steps visit the jupyter notebook here LINK  Isolate Dependent Variables  Select Features  Test Training SplitDecision Tree 1 üå≤   Expand for Steps Taken      Instantiate, Params, Scoring and GridSearch    Fit Model    Best Params and AUC    Extract Scores  Scores:  92% Precision, 92% recall, 92% F1, 97% accuracy and auc.  All scores are past 90% which are VERY strong indicators of a good model.  We will also build a random forest to reduce overfitting, and grid search for the best parameters.Random Forest 1 üèïÔ∏è   Expand for Steps Taken      Instantiate, Params, Scoring and GridSearch    Fit Decision tree model to training data    Save model via Pickle    Best Params, AUC and Evaluate Scores    Get all scores    Pickle to save model training time  Scores:  This model performed outperformed the decision tree model.          95% Precision, 92% recall, 93% F1, 98% accuracy and auc.        As the better model, let‚Äôs use our test set here. Our test set is a portion of the data that we hide, so we can use it to evaluate our model after training it.  Test set scoring:          96% Precision, 92% recall, 94% F1, 98% accuracy and 96% auc.      Our model performed even better in the test set.      We can be confident that the model will perform well on unseen data.      Feature Engineering for 2nd round of Modeling  Our evaluation scores seem to high and may be cause by data leakage.  This happens when we use data which may:          Already exist in test data      Realistically, we may not have access to this data during deployment        During HR‚Äôs daily operations, we may not have immediate access to satisfaction levels.  Average monthly hours may also be too correlated if employees already gave a resignation notice.  Lets apply the ff:          Satisfaction level column is removed      New column ‚Äòoverworked‚Äô will be either 1 (for 175 hrs+ rendered) or 0 (less than 175 hrs).      Isolate new variables      Create test, train split.      Decision Tree 2 üå≤üå≤Based on our new features, let‚Äôs create a second decision tree.   Expand for Steps Taken      Instantiate, Params, Scoring and Grid Search    Fit model to values    Best Params, Best scores  Scores:  The model has great performance despite dropping satisfaction levels and detailed work hours.            model      precision      recall      F1      accuracy      auc                  Decision Tree 1 CV      91.46%      91.69%      91.57%      97.20%      96.98%              Decision Tree 2 CV      85.67%      90.36%      87.89%      95.85%      95.94%      Random Forest 2 üèïÔ∏èüèïÔ∏èWe will also create a 2nd random forest based on our new features.   Expand for Steps Taken      Instantiate, Params, Scoring and Grid Search    Fit model to values    Pickle    Best Params, Best scores, make results    Confusion Matrix  Scores:  As expected, scores lowered with dropped values, however model still works great.            model      precision      recall      F1      accuracy      auc                  Decision Tree 1 CV      91.46%      91.69%      91.57%      97.20%      96.98%              Random Forest 1 CV      94.87%      91.56%      93.18%      97.78%      98.04%              Decision Tree 2 CV      85.67%      90.36%      87.89%      95.85%      95.94%              Random Forest 2 CV      86.64%      88.08%      87.32%      95.76%      96.49%        If AUC is our deciding matrix, random forest 2 performed better.          AUC is useful in our situation some with class imbalance      It is also a good metric when comparing ranking different models.      Random forest 2 is our champion model üëë      Lets evaluate the model with our test set and create a confusion matrix.            model      precision      recall      F1      accuracy      auc                  Random Forest 2 CV      87.12%      90.96%      89.00%      96.26%      94.14%      Recall that:  Top Left: True Positives - Predicted leaving and actually left.  Lower Right: True Negatives - Predicted staying and actually stayed.  Top Right: False Positives - Predicted leaving but stayed  Lower Left: False Negatives - Predicted staying but left.  The model was able to correctly predict the majority of employees leaving (2433).  The next largest quadrant is the True Negatives, meaning the model predicted stays successfully.  Since our model predicted more false positives than false negatives, it is likely to predict leavers, who actually stay. This is good since our model is sensitive to leavers.  Compared to our Linear regression model, this has significantly better performance.Decision Tree Splits and Feature ImportancesThis graph shows the decision tree splits, although a bit cluttered, we can use Feature Importances with a barplot to see which variables were most important in prediction.Decision Tree: Feature Importances for Employee Leaving PredictionRandom Forest: Feature Importances for Employee Leaving PredictionWe can see that he most important predictors on leavers are:  Number of Projects assigned  Last Evaluation (Ranked first in Decision Tree)  Tenure  OverworkedWhat this means:  Although further investigation is needed, we see that the last evaluation is one of the top predictors.          This means that employees are severely impacted by their performance review, or      Low performance reviews eventually lead to leaving:                  whether by resignation, firing, lower pay or other factors.                      Number of projects assigned and overwork are also both top predictors, our earlier findings our supported.  Finally tenure is also one of the most important predictors.          It is important to conduct interviews or follow-up investigation on the groups of individuals that left the company (such as year 4 leavers).      Recommendations and Next Steps for Salifort Motors   Expand for Full Summary/Recap in earlier stages  Insights Based on Model  Findings from EDA  Leaving is related to:      longer working hours    increasing amount of projects    lower satisfaction levels    lack of promotion    lack of increase in salary throughout years    The leavers fall into the ff groups:      Worked less hours and only 2 projects assigned, lower tenures 3 and below    Worked more hours and more projects assigned, (7+ assigned all left)    High satisfaction, but high amount of work hours, lack of promotion or salary increase    Tenure of 4 years, may have experienced impactful changes.    Here are the attributes of those who stayed:      Average of 3-4 projects assigned    Monthly hours rendered 150-255    High satisfaction for tenure 2-4, lower for 5-6 and reaches initial ranges 7 on above.    Findings from Modeling  From the importance features the ff were identified as top leaving predictors:      Number of Projects assigned    Last Evaluation    Tenure    Overworked  Recommendations on Workload  Equally distribute workload between workers, (between 3-4 projects only).  Reduce monthly hours rendered, through automation, improved systems or workflow as overwork is highly correlated to leaves.  Conducting interviews on most time consuming areas of the work will be a huge step.  Make sure new employees understand hourly expectations before hiring.  Reward employees working overtime via pay, benefits or bonus.Recommendations on Evaluation  Identify effect of low evaluation to employed and quitting, is pay reduced? Does this eventually lead to firing?  Avoid reserving high evaluation for 200+ hrs rendered, consider a bonus or incentive system instead.  Utilize the model to predict potential leavers, and create an intervention and reward program for employees.Recommendations on Identifying Causes  Identify via interviews or investigation, what causes lower satisfaction on tenures 5-6, and what caused leaving from year 4.  Consider promoting or increasing salary for individuals on their 4th year (ensure promotion within 5 years).  Conduct internal surveys, FGDs and team meetings to check on employees workload, health, mental and well-being.Improving the Model  Reduce columns of data that will not be realistically always available (such as last evaluation if not conducted often).  Creating unsupervised learning models to identify common traits between employees and leaving.  Specific information instead of categorical (such as salaries) and see if it improves performance.  Test different models or utilize models with better explainability (such the single decision tree) to better identify reasons for leaving.Other Questions that can be addressed  Departments with highest leaves include sales, technical and support, however, this is in proportion as these groups are the largest also.  For further insights or analyses, feel free to contact me.Resources Used  Previous work materials  Google Advanced Data Analytics Certificate course materialsEthical Considerations  Deployment of the model will cause identification of certain individuals who may be likely to resign. With this, the company has great responsibility to maintain fairness when treating its leaving and non-leaving employees. Likewise, the model still has false positive/negative predictions and ultimately, early intervention is required from the company rather than relying on the model.ConclusionThank you for your time reading this output on Salifort Motors. Although the output was lengthy and insights are summarized at the end, more detailed information and analyses are provided throughout the notebook, specifically in the Exploratory Data Analysis (EDA) and model building.If you want similar analysis or have job opportunities for a Data Analyst/Scientist feel free to email me at lotillaryan@gmail.com. Thank you again for your time and have a great day ahead!"
  }
  
]

