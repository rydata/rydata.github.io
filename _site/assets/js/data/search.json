[
  
  {
    "title": "üßëüèª‚Äçü§ù‚ÄçüßëüèΩ HR Analysis on Employee Turnover! Full Data Science Process",
    "url": "/posts/hr-analysis/",
    "categories": "Full Process, Data Science",
    "tags": "data science, cleaning, eda, visualization, logistic regression, decision tree, random forest, feature engineering, recommendations",
    "date": "2023-11-23 00:00:00 +0800",
    





    
    "snippet": "Data Science ProcessIn this project, the following skills were applied: Cleaning, EDA, Logistic Regression, Decision Tree, Random Forest and ReportProject Objective Overview  Central Question: Why ...",
    "content": "Data Science ProcessIn this project, the following skills were applied: Cleaning, EDA, Logistic Regression, Decision Tree, Random Forest and ReportProject Objective Overview  Central Question: Why causes employees to leave the company?  We are tasked to understand the employee turn over in the company.  Based on this, we pull insights and strategical recommendations.  Libraries used include: numpy, pandas, matplotlib, seaborn, sklearn (logistic regression, decision tree, random forest, gridsearch, metrics) and pickle to save the model.  You can access the Jupyter Notebook IPYNB file here LINKProject Findings OverviewOn Employees who left:  Employees are generally overworked with almost double the regular monthly hours.  Those who left either worked less hours and had only 2 projects assigned OR  Are overworked with 255 - 300+ hours and had had more projects 4-7.  Everyone with 7 projects assigned left.  Even highly satisfied employees, with more than 5 years are leaving due to lack of promotion and salary increase.  Those with 4 years in the company, left due to drastic policy change.On Employees who stayed:  Staying employees had an average of 3-4 projects assigned, rendering monthly hours of 150-255 (still high compared to average of 167hrs)  Satisfaction levels were generally high but lowered for tenure/years 5-6.On modeling:  The logistic regression model approach had achieved high scores (79% precision, 82% recall, and f1 of 80%).  The decision tree model and random forest had even better scores and performed similarly (94% precision, 91% recall, f1 of 93%, accuracy and auc 98%).  Feature Engineering was used to drop data which will not be realistically available.  Second round of tree models were created. The random forest is the champion model and scored the ff on the test set: (87% precision, 91% recall, 89% f1, accuracy 96% and auc 94%)Recommendations at the end!Data Cleaning ProcessAfter importing the packages we can do our initial data cleaning  head(), .info, .describe and .columns to better understand our data  rename the columns to snake_case (lowercase and underscores) and correct mispellingsdf0 = df0.rename(columns={'Work_accident': 'work_accident',                          'average_montly_hours': 'average_monthly_hours',                          'time_spend_company': 'tenure',                          'Department':'department'})  .isnull(), .duplicated(), .drop_duplicates() to check for missing values and duplicates  boxplot to check for outliersYou can use"
  },
  
  {
    "title": "Hello World",
    "url": "/posts/hello-world/",
    "categories": "Hello World",
    "tags": "Hello World",
    "date": "2023-06-03 00:00:00 +0800",
    





    
    "snippet": "Hello WorldHello World this is my personal blog.",
    "content": "Hello WorldHello World this is my personal blog."
  }
  
]

